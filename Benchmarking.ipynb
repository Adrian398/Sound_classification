{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ThWFHJDeiyjU"
   },
   "source": [
    "# Benchmarking our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook intends to benchmark the in the CNN_classification created CNN to different state of the art sound classification algorithms. \n",
    "\n",
    "Notes:\n",
    "- all '#free memory space' cells can be ignored if you have 32 gb of RAM or more\n",
    "- we ran all experiments on a server with 128 gb of RAM and Nvidia Tesla P100 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we implement different state-of-the-art audio classification approaches to assess the performance of our CNN. In contrast to deep neural networks, these models are based on hand-crafted features. To this end, we extract 645 features from the spectrogram, namely the arithmetic mean, minimum, maximum, and median value for each frequency. We chose four different baseline models. These comprise two tree-based ensembles, a gradient tree boosting (XGB) and a random forest (RF), as well as a support-vector machine (SVM) and a Gaussian naive Bayes classifier (GNB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E4T48HhJiyja"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy.signal import spectrogram, stft\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, f1_score, balanced_accuracy_score, confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QwtE0UdAiyji"
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 32000\n",
    "SEED = 42\n",
    "VAL_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.2\n",
    "NFTT = 256\n",
    "N_TREES = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QzyBkww4iyjo"
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t2-8UC9Liyjq"
   },
   "outputs": [],
   "source": [
    "x_niko = np.load(\"data/raw/x_niko.npy\")\n",
    "x_adrian = np.load(\"data/raw/x_adrian.npy\")\n",
    "x_toni = np.load(\"data/raw/x_toni.npy\")\n",
    "x_adrian2 = np.load(\"data/raw/x_adrian2.npy\")\n",
    "x_adrian3 = np.load(\"data/raw/x_adrian3.npy\")\n",
    "x_raw = np.concatenate((x_adrian, x_niko,x_toni,x_adrian2,x_adrian3), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#free memory space\n",
    "del x_niko\n",
    "del x_adrian\n",
    "del x_toni\n",
    "del x_adrian2\n",
    "del x_adrian3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "84iDSf5Piyjv"
   },
   "outputs": [],
   "source": [
    "y_niko = np.load(\"data/raw/y_niko.npy\")\n",
    "y_adrian = np.load(\"data/raw/y_adrian.npy\")\n",
    "y_toni = np.load(\"data/raw/y_toni.npy\")\n",
    "y_adrian2 = np.load(\"data/raw/y_adrian2.npy\")\n",
    "y_adrian3 = np.load(\"data/raw/y_adrian3.npy\")\n",
    "y_raw = np.concatenate((y_adrian, y_niko,y_toni,np.squeeze(y_adrian2),y_adrian3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#free memory space\n",
    "del y_niko\n",
    "del y_adrian\n",
    "del y_toni\n",
    "del y_adrian2\n",
    "del y_adrian3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourier transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network architectures for sound classification are designed to classify an acoustic signal based on its frequency spectrum. To obtain this, we decompose each recorded five-second time window into its individual frequencies utilizing the short-time Fourier transformation. This transformation splits a function of time (the sensor readings) into its frequencies. Performing the Fourier Transformation on our one-dimensional raw sensor data returns a two-dimensional spectrogram.\n",
    "\n",
    "In this case we simply load the in the 'CNN_classification created' fouriier transformed data into this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MKL5pB_7iyj1"
   },
   "outputs": [],
   "source": [
    "#fourier transformation\n",
    "x_four = np.load(\"data/fourier/x_four.npz\")\n",
    "\n",
    "#log scale \n",
    "x_log = 10. * np.log10(x_four+np.finfo(float).eps) # from plt.spectrogram\n",
    "x_log = x_log[:,:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#free memory space\n",
    "del x_raw\n",
    "del x_four"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract 645 features from the spectrogram, namely the arithmetic mean, minimum, maximum, and median value for each frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wE4MV36-iykG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645Features created!\n"
     ]
    }
   ],
   "source": [
    "features = np.hstack([np.mean(x_log, axis=2),\n",
    "                      np.std(x_log, axis=2),\n",
    "                      np.median(x_log, axis=2),\n",
    "                      np.min(x_log, axis=2),\n",
    "                      np.max(x_log, axis=2)])\n",
    "\n",
    "print(str(len(features[0])) + \" Features created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#free memory space\n",
    "del x_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jkvy7B6_iykU"
   },
   "outputs": [],
   "source": [
    "# train and test split the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, \n",
    "                                                    y_raw, \n",
    "                                                    test_size=TEST_SPLIT, \n",
    "                                                    stratify=y_raw,\n",
    "                                                    random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#free memory space\n",
    "del features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VNmWRAbJiykc"
   },
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 645 features from the spectrogram, namely the arithmetic mean, minimum, maximum, and median value for each frequency are now inserted into different algorithms. These comprise two tree-based ensembles, a gradient tree boosting (XGB) and a random forest (RF), as well as a support-vector machine (SVM) and a Gaussian naive Bayes classifier (GNB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fkNEaJweiykd"
   },
   "outputs": [],
   "source": [
    "#xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=SEED)\n",
    "#clf = RandomForestClassifier(random_state=SEED)\n",
    "#svc = SVC()\n",
    "#nb = GaussianNB()\n",
    "#model_list = [xgb_model, clf, svc, nb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9rynQXISiykg"
   },
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(n_estimators=N_TREES,\n",
    "                              objective=\"binary:logistic\", random_state=SEED)\n",
    "clf = RandomForestClassifier(n_estimators=N_TREES,\n",
    "                             random_state=SEED)\n",
    "svc = SVC()\n",
    "nb = GaussianNB()\n",
    "model_list = [xgb_model, clf, svc, nb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u9Kfpwnxiykk",
    "outputId": "7f5285e0-186f-422a-9778-fdcbf56b56b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['XGBClassifier', 96.2482024733966, 96.32107023411372, 94.42622950819673, 95.36423841059603, 92.93201738615686]\n",
      "['RandomForestClassifier', 94.40322116767328, 98.55595667870037, 89.50819672131148, 93.81443298969072, 90.97903859169058]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stellaris/anaconda3/envs/Sc_interpreter/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SVC', 84.26229508196721, 100.0, 68.52459016393443, 81.32295719844359, 76.5814414606606]\n",
      "['GaussianNB', 58.34483750359506, 39.12483912483912, 99.672131147541, 56.19223659889094, 25.218060375107747]\n"
     ]
    }
   ],
   "source": [
    "res_list = []\n",
    "for i, model in enumerate(model_list):\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    res = [type(model).__name__,\n",
    "           balanced_accuracy_score(y_test, y_pred)*100,\n",
    "           precision_score(y_test, y_pred)*100,\n",
    "           recall_score(y_test, y_pred)*100,\n",
    "           f1_score(y_test, y_pred)*100,\n",
    "           matthews_corrcoef(y_test, y_pred)*100]\n",
    "    print(res)\n",
    "    res_list.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FiAZXxHuoTpQ"
   },
   "source": [
    "## Create metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics:\n",
    "- Matthews correlation coefficient (MCC) is generally regarded as a good measure for imbalanced data. It takes true positives (instances of correctly classified properly connected plugs), false positives (instances that contain falsely connected plug events but are erroneously classified as properly connected), true negatives (instances of falsely assembled plugs classified as falsely assembled plugs), and false negatives (instances of properly assembled plugs that are erroneously classified as falsely assembled) into account.\n",
    "- Precision reports the fraction of correctly classified correctly assembled plugs among all instances that are classified as correctly assembled, i.e., true positives divided by the sum of true positives and false positives.\n",
    "- Recall indicates the fraction of correctly assembled plugs that are correctly classified (true positives) among all correctly assembled plugs (true positives and false negatives).\n",
    "- F-Measure considers both precision and recall. It is calculated as the harmonic mean of the Precision and Recall criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t7ZVi3Hmiykp",
    "outputId": "fa9ea487-f6e7-4f6f-8733-d7d55c1aca43"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Balanced</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>96.25</td>\n",
       "      <td>96.32</td>\n",
       "      <td>94.43</td>\n",
       "      <td>95.36</td>\n",
       "      <td>92.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>94.40</td>\n",
       "      <td>98.56</td>\n",
       "      <td>89.51</td>\n",
       "      <td>93.81</td>\n",
       "      <td>90.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>SVC</td>\n",
       "      <td>84.26</td>\n",
       "      <td>100.00</td>\n",
       "      <td>68.52</td>\n",
       "      <td>81.32</td>\n",
       "      <td>76.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>58.34</td>\n",
       "      <td>39.12</td>\n",
       "      <td>99.67</td>\n",
       "      <td>56.19</td>\n",
       "      <td>25.22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model  Balanced   Precision  Recall     F1    MCC\n",
       "0           XGBClassifier      96.25      96.32   94.43  95.36  92.93\n",
       "1  RandomForestClassifier      94.40      98.56   89.51  93.81  90.98\n",
       "2                     SVC      84.26     100.00   68.52  81.32  76.58\n",
       "3              GaussianNB      58.34      39.12   99.67  56.19  25.22"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(res_list, columns=[\"Model\", \"Balanced \", \"Precision\", \"Recall\", \"F1\", \"MCC\"]).round(decimals=2)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write all metrics to latex to insert them in our paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3kWokzA3iykz",
    "outputId": "06b62261-214a-471a-c35e-7432cb2c3d43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\begin{tabular}{lrrrrr}\\n\\\\toprule\\n                  Model &  Balanced  &  Precision &  Recall &     F1 &    MCC \\\\\\\\\\n\\\\midrule\\n          XGBClassifier &      96.25 &      96.32 &   94.43 &  95.36 &  92.93 \\\\\\\\\\n RandomForestClassifier &      94.40 &      98.56 &   89.51 &  93.81 &  90.98 \\\\\\\\\\n                    SVC &      84.26 &     100.00 &   68.52 &  81.32 &  76.58 \\\\\\\\\\n             GaussianNB &      58.34 &      39.12 &   99.67 &  56.19 &  25.22 \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.round(decimals=2).to_latex(index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "benchmarking.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
